---
categories:
- 开源
- 感悟
date: 2025-02-21T17:18:21+08:00
description: "大模型是新的人类创造发明的事物，尽管它是从计算机走出来的，但是不止代码，数据集，以及训练后的models，作为用户怎么下载，怎么体验，也就成就了一个平台。这个平台像所有的网络平台一样，依然会形成网络效应。"
keywords:
- Open Source
- Culture
- Reading
- News
tags:
- 每周精选
- 开源之道
title: "非官方观察：DeepSeek 的开放之路系列之三：模型市场Hugging face"
url: ""
authors:
- 「开源之道」·适兕 && 「开源之道」·窄廊
---

![](/images/deepseek-github-readme.png)

本文是系列文章的第三篇，如有需要，可先预习前两篇：

* [非官方观察：DeepSeek 的开放之路系列之一：arXiv](/posts/opensource_engineering/deepseek-open-path-series-0/)
* [非官方观察：DeepSeek 的开放之路系列之二：重识 GitHub](/posts/opensource_engineering/deepseek-open-path-series-1-github.md)

## DeepSeek 仓库的 ReadMe 

大模型并不是软件，尽管软件是它的一部分，GitHub 作为 DeepSeek 开放之路的入口[1]，其训练过程的基础设施源代码在笔者撰写文章的时候，正在逐步释出[2]，正如我们看到的，整个世界都是如火如荼的部署 DeepSeek 的时候，其实是它发布的模型，这些模型的发布就在Hugging face上。

## Hugging face 是什么？

当众多的研究机构、商业组织训练完成了自己的大模型之后，需要有一个展示的地方，目前来说，Hugging face 就是其中最大的一个站点，此外还有魔搭社区[3]等，也就是说，该站点类似于GitHub，不过托管的内容是模型，而不是代码。

模型有个很大的特点，那就是大，动辄几个G，上百个G也是稀松平常。围绕着模型的库、数据集、社交、蒸馏的平台就这样形成了，就像上篇文章一样，笔者写这篇文章也不是介绍HF（Hugging face，下同）的功能，而是功能之外的，如平台、网络效应等。

像Google、Wikipedia一样，这是一个遥远的web站点服务，并不能直接访问。它的域名也比较特殊：[https://huggingface.co/](https://huggingface.co/)。

## HF 成为DeepSeek 发布的默认平台的关键

在大语言模型能够成为我们社会发展的重要关注点，HF的功劳不可没，我们捋一捋HF做的事情：

* 开源 Transformers 库[4]： Hugging Face 最核心的贡献是开源了 Transformers 库。  这个库极大地简化了 Transformer 模型（大语言模型的核心架构）的开发、训练和使用。  在 Transformers 库出现之前，从零开始构建和训练 Transformer 模型非常复杂，需要深厚的专业知识和大量的工程投入。  Transformers 库的出现，将 Transformer 模型的复杂性封装起来，提供了预训练模型、模型组件、训练脚本、评估工具等等， 极大地降低了开发者使用 Transformer 模型的门槛。  即使不具备深厚的机器学习背景，开发者也可以通过 Transformers 库快速上手，进行大语言模型的微调和应用开发。
* Model Hub (模型中心) [5]: Hugging Face 建立了 Model Hub 模型中心，这是一个集中托管和分享预训练模型的平台。  Model Hub 上汇集了数以万计的预训练模型，包括各种规模、各种任务的大语言模型，例如 BERT、GPT、T5、Llama 等等。  开发者可以直接从 Model Hub 上 免费下载 这些预训练模型，并在自己的任务上进行微调或直接使用。  这极大地 节省了从头训练大模型的巨大成本和时间，使得更多组织和个人能够利用先进的大语言模型技术。  Model Hub 也鼓励了模型的 共享和复用，加速了知识的传播和技术的迭代。
* Datasets Hub (数据集中心)[6] : 除了模型，训练大语言模型还需要海量的数据。 Hugging Face 也建立了 Datasets Hub 数据集中心，托管了大量的开源数据集，涵盖各种 NLP 任务和数据类型。  开发者可以方便地从 Datasets Hub 上 获取训练数据，用于模型的微调和评估。  这解决了大语言模型开发中数据获取的难题，进一步降低了开发门槛。
* Accelerate 库： 为了进一步简化大模型的训练和推理，Hugging Face 推出了 Accelerate 库。  Accelerate 库可以帮助开发者 轻松地在不同的硬件 (CPU, GPU, TPU) 和分布式环境 (单机多卡, 多机多卡)  上运行相同的代码，无需修改大量代码即可实现模型的并行训练和推理。  这降低了开发者进行大模型训练和部署的复杂性，使得更多人可以进行更大规模的模型实验和应用开发。

除此关键的特性之外，HF也一直在探索为大模型的开发者和工程师们服务，推出诸如大模型运行体验空间（spaces）、甚至交流沟通渠道HuggingChat等。笔者更为看重的是HF所提供的各类沟通，除GitHub上的star、watch、PR之外，还提供blog、forum等等，就是让人们进行各种沟通，或许这才是人类最强大的地方。

## Hugging face 的发展时间

Hugging Face 的发展历程与大语言模型的兴起紧密交织，可以说 Hugging Face 的关键功能推出，都直接或间接地推动了大语言模型的普及和流行。

以下是一个表格，梳理了 Hugging Face 的一些重要里程碑事件，并在最后一栏里标注了事件与大语言模型流行度崛起的关系：

| **年份** | **月份 (或大致时间)** | **Hugging Face 重要里程碑事件 / 功能推出**                                                                  | **对大语言模型流行度的影响分析**                                                                                                                                                                                                                                                                                                                         |
| :------- | :------------------- | :-------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **2016** |                      | **Hugging Face 公司成立，最初是一款聊天机器人应用**                                                              |  *  虽然最初与大语言模型无直接关系，但为后续转型和技术积累奠定基础。                                                                                                                                                                                                                             |
| **2017** |                      | **开源 PyTorch 版本 `transformers` 库的前身 (`pytorch-pretrained-bert`)**                                          |  *  **初步降低 Transformer 模型使用门槛**：首次以开源库的形式提供预训练 BERT 模型及相关工具，使得研究人员和开发者更容易接触和使用 Transformer 模型，为后续大语言模型的研究和应用奠定技术基础。                                                                                             |
| **2018** | **10月**            | **`pytorch-pretrained-bert` 库正式更名为 `transformers`，并发布 TensorFlow 版本， 成为 **`transformers` 库的正式诞生**  |  *  **进一步降低 Transformer 模型使用门槛，扩大用户群体**：同时支持 PyTorch 和 TensorFlow 两个主流深度学习框架，吸引更广泛的开发者，加速 Transformer 模型在学术界和工业界的普及。                                                                                                                 |
| **2019** |                      | **推出 Model Hub (模型中心) 的早期版本**                                                                     |  *  **初步建立模型共享平台，促进预训练模型传播**： Model Hub 的早期版本开始汇集预训练模型，方便用户查找和下载，初步形成了模型共享和复用的生态，为后续大规模预训练模型的流行奠定基础。                                                                                                                                |
| **2020** |                      | **Transformers 库持续快速迭代，支持更多模型架构 (例如 GPT-2, T5 等) 和任务，功能日益完善**                                |  *  **功能完善，应用场景拓展**：Transformers 库不断扩展功能，支持更多 Transformer 变体架构和 NLP 任务，使得开发者可以使用该库解决更广泛的自然语言处理问题，进一步推动 Transformer 模型成为主流 NLP 模型。                                                                                                   |
| **2020** | **9月**             | **正式发布 Hugging Face Model Hub 平台**                                                                        |  *  **构建大型模型共享社区，加速预训练模型普及**：Model Hub 正式发布，成为一个集中托管、发现和使用预训练模型的中心化平台，极大地促进了预训练模型的共享和传播，降低了获取高质量预训练模型的难度，为后续大语言模型的快速发展和应用爆发奠定了关键基础设施基础。                                                                       |
| **2021** |                      | **推出 Datasets Hub (数据集中心)**                                                                       |  *  **解决数据难题，降低模型训练门槛**： Datasets Hub 的推出，解决了大语言模型训练中数据获取和管理的难题，用户可以方便地查找、下载和使用开源数据集，进一步降低了模型训练的门槛，促进了更多人参与到大语言模型的研究和开发中。                                                                                                                                    |
| **2021** |                      | **Transformers 库支持模型微调 (Fine-tuning) 和模型压缩等功能， 更加易用和高效**                                 |  *  **提升模型应用效率，加速模型落地**： 模型微调功能使得用户可以基于预训练模型快速定制化自己的应用，模型压缩功能则降低了模型部署的成本和难度，进一步推动了 Transformer 模型在各种实际场景中的应用落地。                                                                                                                                   |
| **2022** | **5月**             | **开源 Accelerate 库，简化分布式训练和推理**                                                                   |  *  **突破硬件限制，支持更大规模模型训练和推理**：Accelerate 库的推出，极大地简化了分布式训练和推理的复杂性，使得开发者更容易在多 GPU/多机环境下训练和部署更大规模的模型，为后续更大规模的大语言模型 (例如千亿参数模型) 的出现和普及提供了重要的技术支撑。                                                               |
| **2022 - 至今** |                  | **持续迭代和完善 Transformers 库、Model Hub、Datasets Hub、Accelerate 等平台和工具， 积极拥抱和支持新的大语言模型架构 (例如 Llama, BLOOM, OPT 等)， 并积极推动开源和开放 AI 的理念** |  *  **构建繁荣的 LLM 生态系统，推动技术创新和应用爆发**：Hugging Face  持续的迭代和完善，以及对开源和开放 AI 理念的坚持，构建了一个充满活力和创新力的大语言模型生态系统，吸引了全球大量的开发者和研究者参与其中，共同推动大语言模型技术的快速发展和应用普及，最终促成了 2023 年 ChatGPT 引爆的大语言模型应用浪潮。                                                                                              |
| **2023 - 至今** |                  | **积极发展商业化服务 (例如 Inference API, AutoTrain, Expert Acceleration Program 等)，  将开源技术转化为商业价值， 并反哺开源社区**               |  *  **可持续发展，反哺开源生态**：Hugging Face  在商业上的成功，为其开源项目的持续发展提供了经济支撑， 形成了一个良性循环，使其能够持续投入资源，维护和扩展开源项目，并为社区提供更好的服务，进一步巩固了其在大语言模型领域的领导地位，并推动整个行业更加健康和可持续地发展。                                                                                               |

## 为什么是Hugging Face？

还是那句话，作为解释者，有着无数的观点，但是我们可以看到自从2017年，模型的思路提出来之后，Hugging face就一直在该领域寻求突破，并在2022年后的大爆发做好了准备。Hugging Face 的崛起与大语言模型流行度崛起是互相促进、相辅相成的关系。

*   **Hugging Face 通过持续推出关键的开源项目和平台 (Transformers, Model Hub, Datasets Hub, Accelerate 等)， 极大地降低了大语言模型的开发、训练、部署和使用门槛，使得更多开发者和研究人员可以更容易地接触和利用大语言模型技术。**  这为大语言模型的普及和应用爆发奠定了坚实的技术基础。

*   **Hugging Face 构建了一个开放、协作、繁荣的大语言模型生态系统， 吸引了全球大量的开发者和研究者参与其中，共同推动技术创新，加速了应用落地， 最终促成了大语言模型的流行和普及。**

*   **大语言模型的流行反过来也促进了 Hugging Face 的崛起和发展， 带来了更多的用户、社区贡献、商业机会，  使其能够持续投入开源项目的建设，  形成了一个良性循环，并最终成为了大语言模型领域的领导者。**

可以说，Hugging Face  不仅仅是一个公司，更是一个 **大语言模型普及浪潮中的关键推动者和基础设施建设者**。  它的贡献，极大地加速了大语言模型技术的发展和应用，并深刻地改变了自然语言处理和人工智能领域的格局。

## Hugging face 做到了什么？

深刻理解并满足了 AI 社区的核心需求：填补了预训练模型共享和复用的空白： 在 Hugging Face 出现之前，预训练模型虽然已经兴起，但缺乏一个中心化的、易于访问和共享的平台。研究人员和开发者往往需要自行寻找、下载和管理模型，过程繁琐且效率低下。 Hugging Face Model Hub 的诞生，恰好填补了这个空白，它提供了一个集中托管、发现和使用预训练模型的平台，极大地简化了模型的获取和复用流程。降低了 Transformer 模型的使用门槛： Transformer 模型是大语言模型的核心架构，但其复杂性也让许多开发者望而却步。 Hugging Face 开源的 Transformers 库，将 Transformer 模型的复杂性封装起来，提供了易用的 API、预训练模型、工具和文档，极大地降低了开发者使用 Transformer 模型的门槛，使得更多人能够快速上手并应用于自己的项目中。构建了开放、协作的 AI 社区： Hugging Face 不仅仅是一个平台，更是一个 开放、协作的 AI 社区。它鼓励模型、数据集和代码的共享，促进开发者之间的交流和合作。 这种社区驱动的模式，吸引了全球大量的开发者和研究者参与其中，共同推动 AI 技术的发展。

此外，最重要的是抓住了大语言模型爆发的历史机遇：

* Transformer 架构成为主流： Transformer 架构在自然语言处理领域取得了巨大成功，并成为大语言模型的核心。 Hugging Face 很早就All-in Transformer 技术，并围绕 Transformer 库构建了其核心生态，这使得它在 Transformer 模型爆发的浪潮中占据了先发优势。 
* 预训练+微调范式兴起： 预训练模型+微调的范式成为训练大语言模型的标准方法。 Hugging Face 的模型中心和 Transformers 库完美地契合了这种范式，模型中心提供了丰富的预训练模型，Transformers 库则提供了便捷的微调工具，使得开发者可以轻松地利用预训练模型进行下游任务的迁移学习。
开源和开放成为趋势： 随着 AI 技术的日益普及，开源和开放成为了大趋势。 Hugging Face 从一开始就坚持开源和开放的理念，其核心库和平台都是开源的，鼓励社区贡献，这符合了时代潮流，也赢得了开发者和研究者的广泛认可和支持。

HF 是资本驱动的公司，随着持续的投入，不断的融资，保持着持续创新和快速迭代，构建完善的生态系统：

* 不断扩展和完善 Transformers 库： Hugging Face 持续投入资源， 不断扩展和完善 Transformers 库的功能，使其能够支持最新的模型架构、最前沿的研究方向，并提供更高效、更易用的工具。 这种持续的迭代和创新，保证了 Transformers 库始终保持领先地位。
* 积极拓展平台功能和服务： Hugging Face 不断拓展其平台的功能和服务，例如推出 Datasets Hub (数据集中心) 解决数据难题， Spaces (模型演示空间) 方便用户分享模型应用， Accelerate (加速库) 简化分布式训练和推理， Inference API (推理API) 提供模型部署服务等等。 这些不断扩展的功能和服务，构建了一个完善的 AI 生态系统，满足了开发者从模型开发、训练、部署到应用的全方位需求。
* 强大的社区运营和用户支持： Hugging Face 非常重视社区运营和用户支持， 积极维护开源社区，举办各种活动，提供高质量的文档和教程，及时响应用户反馈，解决用户问题。 这种积极的社区运营和用户支持，建立了强大的用户粘性和社区凝聚力。

## 文化的重要作用

文化起着关键的作用，没有任何一家创业公司不依靠强大的文化和价值观来脱颖而出的，HF也不能例外，我们根据网上的内容整理为：

* 坚持 "AI democratize" 的理念[7]： Hugging Face 一直秉持着 "AI democratize" (AI 平民化) 的理念，致力于让更多人能够平等地接触、使用和构建 AI 技术。 其开源、开放、社区驱动的模式，都体现了这一理念， 这也使得 Hugging Face 不仅仅是一个商业公司，更像是一个推动 AI 普及的社会力量，更容易获得用户的认同和尊重。
* 重视用户和社区贡献： Hugging Face 非常重视用户和社区的贡献， 积极鼓励用户参与开源项目的开发，分享模型和数据集，贡献代码和文档。 对于社区的贡献，Hugging Face 也给予积极的认可和回报，例如社区贡献者计划、模型和数据集的作者署名等等。 这种重视用户和社区贡献的企业文化，激发了社区的活力和创造力。
* 开放透明的企业文化： Hugging Face 的企业文化非常 开放透明，其代码开源，平台开放，积极分享技术和经验。 这种开放透明的企业文化，赢得了社区的信任，也吸引了更多优秀人才加入 Hugging Face 团队。

## 提供一个大模型服务都需要什么软硬件环境？

相比于我们过去见过的软件服务，大模型多了一个过程，那就是服务框架或推理引擎，大家所熟知的硅基流动就提供这样的商业服务[8]。DeepSeek 则将其模型默认下载体验使用的支持的项目分别是：

* vLLM[9]
* SGLang[10]

这些都是可以让大模型运行起来，并为我们用户提供一些操作的界面，比如通过聊天来解答一些问题等。

大模型的训练过程以及训练完成后对用户提供服务，都需要用到大量的开源项目，接下来的系列之四，笔者将会探索DeepSeek都用到了哪些重要的开源项目，它所站的肩膀——巨人都是谁。

## 参考资料

1. https://github.com/deepseek-ai/DeepSeek-V3
2. https://github.com/deepseek-ai/open-infra-index
3. https://modelscope.cn/home
4. https://huggingface.co/docs/transformers/index
5. https://huggingface.co/models
6. https://huggingface.co/datasets
7. https://odsc.medium.com/the-evolution-of-hugging-face-and-its-role-in-democratizing-ai-76f19af6d374
8. https://siliconflow.cn/zh-cn/siliconllm
9. https://github.com/vllm-project/vllm?tab=readme-ov-file
10. https://github.com/sgl-project/sglang

## 关于作者

### 「开源之道」·适兕

![](/public/kuosi-face-of-os.png)「发现开源三部曲」（[《开源之迷》](posts/book-of-open-source/the-fascinating-of-open-source/)，《开源之道》《开源之思》。）、[《开源之史》](posts/history-of-open-source/summary/)作者，「开源之道：致力于开源相关思想、知识和价值的探究、推动」主创，Linux基金会亚太区开源布道者，TODO Ambassadors & OSPOlogyLive China Organizer，云计算开源产业联盟OSCAR（中国信息通信研究院发起）个人开源专家，OSPO Group 联合发起人。

### 「开源之道」·窄廊

![](/public/zhailang.jpg) 来自于大语言模型的 Chat，如DeepSeek R1、Gemini 2.0 Flash thinking expermental、ChatGPT 4o、Grok3、甚至整合类应用 Monica等， 「开源之道」·窄廊 负责对话、提出问题、对回答进行反馈等操作。
